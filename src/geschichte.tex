\section{Die Geschichte künstlicher neuronaler Netze}\label{sec:die-geschichte}
Die Forschung an künstlichen neuronalen Netzen hatte bereits in den 1940er Jahren ihre Anfänge und wurde durch viele verschiedene Wissenschaftler betrieben.\\
1943 veröffentlichten \textsc{Warren~McCulloch} und \textsc{Walter~Pitts} eine Arbeit\footcite{mcculloch} in der sie behaupteten, dass die von ihnen beschriebenen Netze so gut wie jede gesuchte Funktion berechnen könnten.
Kurz danach nannten sie eine mögliche Anwendungsmöglichkeit im Gebiet der Erkennung von räumlichen Mustern.\\
In den Jahren nach der Veröffentlichung \textsc{McCulloch} und \textsc{Pitts}’ Arbeit wurde viel an den Netzen geforscht und auch verändert.
Unter anderem wurde 1949 vom Psychologen \textsc{Donald~O.~Hebb} die klassische \textit{Hebb'sche~Lernregel} aufgestellt.
Diese Regel gilt als Grundlage für die meisten anderen Lernregeln und ist nicht nur auf künstliche neuronale Netze anwendbar.\\
\textsc{Frank~Rosenblatt} und \textsc{Charles~Wightman} begannen 1957 am MIT mit der Entwicklung des \textit{Perceptron}s, ein einfaches neuronales Netzwerk, das sie in Form des Neurocomputers \textit{Mark~I~Perceptron} implementierten.
Der \textit{Mark~I~Perceptron} konnte bereits eine Ziffer in einem $20 \times 20$ Pixel Bild erkennen.\\
1960 entwarfen \textsc{Bernard~Widrow} und \textsc{Marcian~E.~Hoff} die \textit{Deltaregel} beziehungsweise den \textit{LMS-Algorithmus}, mit dem das ebenfalls von \textsc{Widrow} und \textsc{Hoff} entworfene \textit{ADALINE}~(\textit{Adaptive~Linear~Neuron}) arbeitete.
Es ist ein System, das bereits kommerziell als Echo-Unterdrückung in Analogtelefonen eingesetzt wurde.\\
Als 1969 \textsc{Marvin~Minsky} und \textsc{Seymour~Papert} eine mathematische Analyse des \textit{Perceptron}s veröffentlichten, kam die Forschung im Bereich der künstlichen Intelligenz vollständig zum Erliegen.
Sie hatten in ihrem Buch über \textsc{Frank~Rosenblatt}s einfaches \textit{Perceptron}-Modell einige schwerwiegende Mängel festgestellt, woraufhin viele Forschungsgelder gestrichen wurden.
Wegen der knappen Gelder wurde nur noch wenig am Thema weitergeforscht, was \textsc{Paul Werbos} 1974 dazu veranlasste, das schon vorher bestehende \textit{Backpropagation~of~Error}-Lernverfahren auf neuronale Netze anzuwenden.
Dieses Verfahren wurde jedoch erst 10 Jahre nach seiner Veröffentlichung so wichtig, dass es an weiterer Aufmerksamkeit gewann.\\
Ab den 1980er Jahren nahm der amerikanische Wissenschaftler \textsc{John~Hopfield} sehr viel Einfluss auf das mathematisch-informatisch geprägte Thema, unter anderem indem er 1982 seine \textit{Hopfieldnetze} vorstellte.
Die \textit{Hopfieldnetze} sind eine Art von neuronalen Netzen, die sich an den Gesetzen des Magnetismus in der Physik orientieren.
Außerdem veröffentlicht \textsc{Hopfield} 1985 einen Artikel, in dem er eine Lösung des \textit{Travelling~Salesman~Problem}s mithilfe seiner \textit{Hopfieldnetze} beschreibt.
\textsc{Minsky}s Vorhersagen von 1969 stellten sich als größenteils falsch heraus und das Thema gewann die anfängliche Aufmerksamkeit zurück.\\
Von 1980 bis heute hat sich das Thema der künstlichen Intelligenz rasant weiterentwickelt und ist heute bedeutsamer als je zuvor.
~\footcite[][Abschnitt 1.2]{kriesel}